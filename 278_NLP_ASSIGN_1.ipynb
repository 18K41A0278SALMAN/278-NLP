{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "278-NLP-ASSIGN-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFw6aiZ3Q3OdAVOr0zFfxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18K41A0278SALMAN/278-NLP/blob/main/278_NLP_ASSIGN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIrh4ejwkomL",
        "outputId": "fae22344-4e39-4564-fd8c-b8994d52b298"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ol0jKgUl6xj"
      },
      "source": [
        "1)Sentence Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIbExnIQlUJp"
      },
      "source": [
        "text=\"\"\"\n",
        "Are you fascinated by the amount of text data available on the internet? Are you looking for ways to work with this text data but aren’t \n",
        "sure where to begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-SJnOVfmf4t"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "p1=''\n",
        "p1=sent_tokenize(text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfeLcEfHm3S-",
        "outputId": "eb57f00c-57cc-49b0-97fd-6c11b9ad36d2"
      },
      "source": [
        "for s in p1:\n",
        "  print(s)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Are you fascinated by the amount of text data available on the internet?\n",
            "Are you looking for ways to work with this text data but aren’t \n",
            "sure where to begin?\n",
            "Machines, after all, recognize numbers, not the letters of our language.\n",
            "And that can be a tricky landscape to navigate in machine learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3yyMYjvn6Ir"
      },
      "source": [
        "2)Word spitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnZRKdw4n_d4",
        "outputId": "e9a8b4d4-7f37-4d09-effb-d3545e878c3a"
      },
      "source": [
        "for i in range(len(text)):\n",
        "  i=text.split()\n",
        "\n",
        "\n",
        "print(i)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are', 'you', 'fascinated', 'by', 'the', 'amount', 'of', 'text', 'data', 'available', 'on', 'the', 'internet?', 'Are', 'you', 'looking', 'for', 'ways', 'to', 'work', 'with', 'this', 'text', 'data', 'but', 'aren’t', 'sure', 'where', 'to', 'begin?', 'Machines,', 'after', 'all,', 'recognize', 'numbers,', 'not', 'the', 'letters', 'of', 'our', 'language.', 'And', 'that', 'can', 'be', 'a', 'tricky', 'landscape', 'to', 'navigate', 'in', 'machine', 'learning.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkq2JQ9_or_D"
      },
      "source": [
        "3)Stemming \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_WXkcAo22O"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuvR3K-opQWG"
      },
      "source": [
        "wrds=[\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]\n",
        "n1=PorterStemmer()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6VM7OQXppgH",
        "outputId": "21d4ab02-110a-46ef-def4-ba44bd21c859"
      },
      "source": [
        "for i in wrds:\n",
        "  print(n1.stem(i))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "have\n",
            "corriendo\n",
            "at\n",
            "wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_giSIRWp0Zs"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn9mGMv5rgpV",
        "outputId": "74c82447-2d73-489c-9fbf-8dba9897b0bd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIjsTs_rp742"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "l1=WordNetLemmatizer()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dhuZPkiqcsv",
        "outputId": "16adad44-ddfc-4c81-ded5-6ca72d7e8339"
      },
      "source": [
        "for j in wrds:\n",
        "  print(l1.lemmatize(j))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "trouble\n",
            "troubling\n",
            "troubled\n",
            "having\n",
            "Corriendo\n",
            "at\n",
            "wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPzNoWokrz0R"
      },
      "source": [
        "4)Finding Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6mevc-ysAUl"
      },
      "source": [
        "text1=\"\"\"\n",
        "“The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, \n",
        "and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove \n",
        "the word if it exits in the list of stop words provided by NLTK.”\n",
        "\"\"\""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV_2rLRdtXmf",
        "outputId": "e57ff619-22be-4543-f1e3-df8769d15d66"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKS8Jd1dsNXR"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "words=word_tokenize(text1)\n",
        "l1=[]\n",
        "for w in words:\n",
        "  if w not in stop_words:\n",
        "    l1.append(w)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnGtoXMXt_RK",
        "outputId": "6b6921fe-558b-4449-ce6d-40f88d9f7104"
      },
      "source": [
        "print(l1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['“', 'The', 'NLTK', 'library', 'one', 'oldest', 'commonly', 'used', 'Python', 'libraries', 'Natural', 'Language', 'Processing', '.', 'NLTK', 'supports', 'stop', 'word', 'removal', ',', 'find', 'list', 'stop', 'words', 'corpus', 'module', '.', 'To', 'remove', 'stop', 'words', 'sentence', ',', 'divide', 'text', 'words', 'remove', 'word', 'exits', 'list', 'stop', 'words', 'provided', 'NLTK', '.', '”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZdSLbnUuRGX"
      },
      "source": [
        "5)printing frequency of each word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3ThwoxauWZy",
        "outputId": "d0acd4a3-08ad-402f-c485-ee291e77c8b2"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "t1=[t for t in text1.split()]\n",
        "f1=FreqDist(t1)\n",
        "l1=[(k,v) for k, v in f1.items()]\n",
        "\n",
        "print(l1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('“The', 1), ('NLTK', 2), ('library', 1), ('is', 1), ('one', 1), ('of', 3), ('the', 5), ('oldest', 1), ('and', 3), ('most', 1), ('commonly', 1), ('used', 1), ('Python', 1), ('libraries', 1), ('for', 1), ('Natural', 1), ('Language', 1), ('Processing.', 1), ('supports', 1), ('stop', 4), ('word', 2), ('removal,', 1), ('you', 2), ('can', 2), ('find', 1), ('list', 2), ('words', 4), ('in', 2), ('corpus', 1), ('module.', 1), ('To', 1), ('remove', 2), ('from', 1), ('a', 1), ('sentence,', 1), ('divide', 1), ('your', 1), ('text', 1), ('into', 1), ('then', 1), ('if', 1), ('it', 1), ('exits', 1), ('provided', 1), ('by', 1), ('NLTK.”', 1)]\n"
          ]
        }
      ]
    }
  ]
}